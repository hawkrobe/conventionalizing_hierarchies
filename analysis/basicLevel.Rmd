---
title: "Basic-level emergence"
output:
  pdf_document: default
  html_notebook: default
  html_document: 
    smart: false
    
---

# Import libraries

```{r results="hide"}
library(tidyverse)
library(ggthemes)
library(lme4)
```

# Import data  

Import, filter out nonConvergers, pull in condition information

```{r results="hide"}
raw_clicks = read_delim('../data/experiment1/clickedObj/allClicks.csv', '\t')
raw_drops = read_delim('../data/experiment1/drop/allDrops.csv', '\t')
incompletes <- (raw_clicks %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  filter(n < 90))$gameid

masterWordIDLookup <- read_delim('../data/experiment1/postTest_word/allWordPostTest.csv', '\t') %>%
  group_by(gameid) %>%
  mutate(wordID = paste0('word', as.numeric(factor(target)))) %>%
  rename(text = target) %>%
  select(gameid, text, wordID) %>%
  distinct()

masterGameIDLookup <- raw_clicks %>%
  mutate(id = paste0('game', as.numeric(factor(gameid)))) %>%
  select(gameid, id) %>%
  distinct()
```

Filter out incompletes & compute cumulative accuracy. We also divide into quarters to compare games that ran different amounts of trials.

```{r}
d <- raw_clicks %>%
  filter(!(gameid %in% incompletes)) %>%
  mutate(acc = ifelse(correct == 'true', 1, 0)) %>%
  group_by(gameid) %>%
  mutate(quarter = floor((trialNum - 1) / (last(trialNum)/4))) %>%
  mutate(cumAcc = cumsum(acc)) %>%
  mutate(overallAcc = last(cumAcc)/last(trialNum)) %>%
  left_join(raw_drops, by = c('gameid', 'trialNum', 'intendedName')) %>%
  select(-ends_with('y'), -ends_with('x'), -correct) %>%
  left_join(masterWordIDLookup) %>%#, by = c('gameid', 'text'))  
  left_join(masterGameIDLookup)

# Exclude people who are below 50% in final quarter
nonConvergers <- (d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75))$gameid
    
cat('excluded', length(nonConvergers), 'games that never converged')
d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75) %>%
  group_by(condition) %>%
  tally()
```

## Number games per condition

```{r}
d %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  group_by(condition) %>%
  summarize(n = length(n))
```

## Write out for BDA

Want to run these webppl models in parallel, so the input data should be in separate files, easily indexed from the command-line... 

```{r}
gameIDs = unique((d %>% filter(!(gameid %in% nonConvergers)))$id)
for(i in gameIDs) {
  toWrite = d %>% 
    ungroup() %>%
    filter(id == i) %>%
    select(-gameid, -text, -condition, -contextType, -acc, -quarter, -cumAcc, -overallAcc, -timeFromRoundStart)  
  write_csv(toWrite, path = paste0('../models/bdaInput/', i, '.csv'))
}
```

# Behavioral Results 

## Accuracy distributions by quartile of game

So we can clearly see the distributions... 

```{r}
d %>% 
  group_by(gameid, quarter) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = percentCorrect)) +
    geom_histogram(bins = 10) +
    theme_few() + 
    guides(color = FALSE) +
    facet_wrap(~ quarter) 
```

We see a bimodal distribution where some people never converge (we'll exclude these for lexicon analyses).

## Overall accuracy over time

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = trialNum, y = percentCorrect)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("accuracy") +
    ylim(0,1)
```

## *Individual* cumulative accuracy curves over time

Here we see very clearly the different pairs separate out (some never converge)

```{r}
ggplot(d, aes(x = trialNum, y = cumAcc, group = gameid)) +
  geom_line() +
  theme_few() + 
  guides(color = FALSE) +
  ylab("cumulative accuracy")
```

## Accuracy by condition

```{r}
d %>% 
  group_by(condition, quarter) %>%
  summarize(percentCorrect = mean(acc), se = sd(acc)/sqrt(length(acc))) %>%
  ungroup() %>%
  mutate(condition = ifelse(condition == 'intermediateOnly', 'pure intermediate',
                            ifelse(condition == 'mixedLower', 'mixed', 'pure subordinate'))) %>%
  ggplot(aes(x = quarter, y = percentCorrect, color = condition)) +
    geom_line() +
    geom_errorbar(aes(ymin = percentCorrect - se, ymax = percentCorrect + se), width = 0.01) +
    theme_few() +
    #geom_smooth(method = 'loess') +
    scale_color_colorblind() +
    theme(    
      legend.position="top"
    ) +
    ylim(0,1)

ggsave('../writing/cogsci18/figures/accuracyByCondition.pdf', width = 5, height = 4)
```


The overall increase is significant... 

```{r}
summary(glmer(acc ~ condition + trialNum + (1 + trialNum | gameid), family = 'binomial', data = d))
```

## Accuracy by contextType

Within the mixed condition, you might expect slower improvement in sub trials?

```{r}
d %>% 
  filter(condition == 'mixedLower') %>%
  group_by(gameid, contextType, quarter) %>%
  summarize(meanAcc = mean(acc)) %>%
  group_by(contextType, quarter) %>%
  summarize(meanAcc = mean(meanAcc)) %>%
  ggplot(aes(x = quarter, y = meanAcc, color = contextType)) +
    geom_line() +
    theme_few() 

d %>% 
  filter(condition == 'mixedLower') %>%
  group_by(gameid, contextType, quarter) %>%
  summarize(meanAcc = mean(acc)) %>%
  spread(contextType, value = meanAcc) %>%
  mutate(diff = basic - sub) %>%
  group_by(quarter) %>%
  summarize(meanDiff = mean(diff), se = sd(diff)/sqrt(length(diff))) %>%
  ggplot(aes(x = quarter, y = meanDiff)) +
    geom_line() +
    geom_errorbar(aes(ymax = meanDiff + se, ymin = meanDiff - se), width = 0) +
    theme_few() + 
    theme(aspect.ratio = 1) +
    ylim(0,0.2) +
    ylab("mean accuracy difference (intermediate - sub)") +
    ggtitle("accuracy gap between trial types in mixed condition")
```

## Reaction times

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(RT = mean(timeFromRoundStart)) %>%
  ggplot(aes(x = trialNum, y = RT/1000)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("reaction time (seconds)")
```

```{r}
summary(lmer(timeFromRoundStart ~ trialNum + (1 | gameid),  data = d))
```

# Post-test results

```{r}
postTest_word = read_delim('../data/experiment1/postTest_word/allWordPostTest.csv', '\t') %>%
  mutate_each(funs(ifelse(. == "true", 1, 0)), 
              -iterationName, -gameid, -time, -target, -finalRole, -eventType) %>%
  gather(object, meaning, blueSquare1:stripedCircle2) %>%
  mutate(blue = grepl('blue', object),
         red = grepl('red', object),
         striped = grepl('striped', object),
         spotted = grepl('spotted', object),
         circle = grepl("Circle", object),
         square = grepl("Square", object)) %>%
  right_join(d %>% filter(!(gameid %in% nonConvergers))) %>%
  select(iterationName:square, condition) %>%
  group_by_at(vars(-condition)) %>%
  summarize(condition = first(condition)) %>%
  rename(text = target) %>%
  left_join(masterWordIDLookup) %>%
  left_join(masterGameIDLookup)

length(unique(postTest_word$gameid))
```

## Consistency across two post-tests?

TODO: read this in file-by-file to convert headers to wordIDs...

```{r results="hide"}
postTest_obj <- read_delim('../data/experiment1/postTest_object/allObjectPostTest.csv', '\t') %>%
  mutate_each(funs(ifelse(. == "true", 1, 0)), 
              -iterationName, -gameid, -time, -target, -finalRole, -eventType) %>%
  gather(object, meaning, blueSquare1:stripedCircle2) %>%
  mutate(blue = grepl('blue', object),
         red = grepl('red', object),
         striped = grepl('striped', object),
         spotted = grepl('spotted', object),
         circle = grepl("Circle", object),
         square = grepl("Square", object)) %>%
  filter(!(gameid %in% nonConvergers)) %>%
  right_join(d %>% filter(!(gameid %in% nonConvergers))) %>%
  select(iterationName:square, condition) %>%
  group_by_at(vars(-condition)) %>%
  summarize(condition = first(condition))
```

## Result 1: Vocab size by condition.

```{r}
lexiconSize <- postTest_word %>%
  group_by(gameid, finalRole, text, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  tally() %>%
  group_by(condition, gameid) %>%
  summarize(vocabSize = mean(n, na.rm = T)) 

lexiconSize %>%
  group_by(condition) %>%
  summarize(se = sd(vocabSize)/sqrt(length(vocabSize)), vocabSize = mean(vocabSize)) %>%
  arrange(vocabSize) %>%
  mutate(condition = factor(condition, levels = unique(condition))) %>%
  ggplot(aes(x = condition, y = vocabSize)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = vocabSize + se, ymin = vocabSize - se), width = 0) +
    theme_few()

ggsave('../writing/cogsci18/figures/lexiconSize.pdf', height = 3.5, width = 5)
```

```{r}
summary(lm(vocabSize ~ condition, data = lexiconSize))
```

## How many abstract vs. specific terms overall?

```{r}
postTest_word %>% 
  group_by(gameid, finalRole, text, condition) %>%
  summarize(specific = sum(meaning) == 1, abstract = sum(meaning) > 1) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numAbstract = sum(abstract), numSpecific = sum(specific)) %>%
  group_by(condition) %>%
  summarize(numAbstract_m = mean(numAbstract), numSpecific_m = mean(numSpecific),
            numAbstract_se = sd(numAbstract)/sqrt(length(numAbstract)), 
            numSpecific_se = sd(numSpecific)/sqrt(length(numSpecific))) %>%
  gather(metric, value, numAbstract_m:numSpecific_se) %>%
  separate(metric, c('wordType', 'measure')) %>%
  spread(measure, value) %>%
  mutate(wordType = ifelse(wordType == 'numAbstract', '# abstract', '# specific')) %>%
  ggplot(aes(x = wordType, y = m)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0) +
    facet_wrap(~ condition) +
    theme_few() 

ggsave('../writing/cogsci18/figures/lexiconContent.pdf', width = 5, height = 4)
```

## Proportion of specific & abstract within single lexicon?

```{r}
postTest_word %>% 
  group_by(gameid, finalRole, text) %>%
  filter(meaning == 1) %>%
  summarize(subordinate = sum(meaning) == 1,
            basic = (sum(meaning) == 2 & 
                       (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
  group_by(gameid, finalRole) %>%
  summarize(numSub = sum(subordinate),
            numBasic = sum(basic)) %>%
  left_join(d) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>%
  ggplot(aes(x = numSub, y = numBasic)) +#, color = numSub > 0 & numBasic > 0)) +
    geom_jitter(width = .3, height = .3, size = 2)  +
    facet_grid(~ condition) +
    theme_few() +
    xlab("# subordinate meanings") +
    ylab("# basic meanings") +
    theme(aspect.ratio=1) 
  #guides(color=FALSE)
  
ggsave("../writing/cogsci18/figures/fullLexiconReport.pdf")
# postTest %>% 
#   group_by(gameid, finalRole, label) %>%
#   summarize(subordinate = sum(meaning) == 1,
#             basic = 
```

What is modal response in each condition?

```{r}
postTest_word %>% 
  group_by(gameid, finalRole, text) %>%
  filter(meaning == 1) %>%
  summarize(subordinate = sum(meaning) == 1,
            basic = (sum(meaning) == 2 & 
                       (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
  group_by(gameid, finalRole) %>%
  summarize(numSub = sum(subordinate),
            numBasic = sum(basic)) %>%
  left_join(d) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>%
  group_by(condition, numSub, numBasic) %>%
  tally() %>%
  group_by(condition) %>%
  mutate(pct = n/sum(n)) %>%
  select(-n) %>%
  filter(pct == max(pct))
```

## Entropy of specific vs. abstract distribution within lexicon

How many objects does each label correspond to (i.e. how many meanings at sub-level vs. basic-level vs. super-level)

```{r}
postTest_word %>%
  group_by(gameid, finalRole, condition) %>%
  filter(sum(meaning) < 16) %>%
  group_by(gameid, finalRole, text, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  mutate(numMeanings = length(numObjects), abstract = numObjects > 1) %>%
  group_by(gameid, finalRole, abstract, condition) %>%
  summarize(pct = n()/mean(numMeanings)) %>%
  #ungroup() %>%
  group_by(condition) %>%
  complete(gameid, finalRole, abstract, fill = list(pct = 0)) %>%
  spread(abstract, pct) %>%
  mutate(entropy = -`TRUE`*log(`TRUE`) -`FALSE`*log(`FALSE`),
         entropy = ifelse(is.na(entropy), 0, entropy)) %>%
  group_by(condition) %>%
  summarize(m = mean(entropy), se = sd(entropy)/sqrt(length(entropy))) %>%
  ggplot(aes(x = condition, y = m)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0) +
    #facet_wrap(~ condition) +
    theme_few()
```

## How often do players align on meanings?

On average, pairs exactly match on about 50% of the meanings they mark... 

Just overlap of matrix? 

```{r}
postTest_word %>%
  ungroup() %>%
  select(-time, -eventType, -blue, -red, -striped, -spotted, -circle, -square) %>%
  spread(finalRole, meaning) %>%
  filter(listener > 0 | speaker > 0) %>%
  group_by(gameid, text, condition) %>%
  summarize(match = mean(listener == speaker)) %>%
  group_by(gameid, condition) %>%
  summarize(numMatching = mean(match)) %>%
  ggplot(aes(x = numMatching)) +
    geom_histogram(binwidth = .2) +
    #geom_vline(aes(xintercept = mean(numMatching))) +
    xlim(-0.1,1.1) + 
    theme_few() +
    xlab('% matching') +
  facet_wrap(~ condition)
```

But note that pairs that didn't technically align that well on the post-test could still perform pretty well if one partner simply has a stricter meaning than the other but the difference is never relevant. 

When people fail to perfectly align, do they do so in a predictable way? (e.g. one meaning a subset of the other?)

```{r}
```


# Some questions

1. Is this data 'valid' & reliable? Does it allow us to systematically examine the real lexica that are formed? Or is it 'too hard' for many turkers, according to some criterion, so that we're mostly just seeing noise? Collect more data to estimate this better? Redesign task (e.g. only draw targets from half of heirarchy; make it much longer?)

2. Do they use words appropriately (i.e. not just to literally mean what they say it means but also in context where it's useful)?

3. What is confusion matrix of objects for diff. words? May see clustering within categories if participants haven't yet aligned on 

4. What is sequence of formation (first occurrence: basic-level, subordinate?)

5. Pragmatic effects (e.g. words on subordinate trials being extended to basic-level)

6. Do people align better when the play longer?

7. What stats to use for coexistence of basic- and sub-?

# Model analysis

Import lexical posteriors

```{r}
library(rjson)

sigmoid = function(x) {
  return(1/(1+exp(-x)))
}

words = paste0('word', seq(1:16))
objects = as.character(c('blueSquare1', 'blueSquare2', 'redSquare1', 'redSquare2',              
            'spottedCircle1', 'spottedCircle2', 'stripedCircle1', 'stripedCircle2'))
result <- fromJSON(file = "../models/game1.json")
speakerPosterior = cbind(expand.grid(object=objects, wordID=words,stringsAsFactors=T),
                         data.frame(qt1 = result$speakerHyp1mu$data,
                                    qt2 = result$speakerHyp2mu$data,
                                    qt3 = result$speakerHyp3mu$data,
                                    qt4mu = result$speakerHyp4mu$data,
                                    qt4sigma = result$speakerHyp4sigma$data))
speakerPosterior %>% 
  left_join(postTest_word %>% filter(id == 'game1') %>% filter(finalRole == 'speaker')) %>%
  mutate(finalPrediction = 1 - pnorm(0, mean = qt4mu, sd = qt4sigma)) %>%
  ggplot(aes(x = finalPrediction, y = meaning)) +
    geom_point()
```

```{r}
BDAparams <- read_csv('../models/bdaOutput/lexicalInferenceParams.csv') %>%
  mutate(sample = row_number()) %>%
  gather(param, value, -sample) %>%
  separate(param, into = c('word', 'object', 'trialNum'))
```

```{r}
read_csv('../models/bdaOutput/lexicalInferenceDrifts.csv') %>%
  mutate(sample = row_number())  %>%
  ggplot(aes(x = driftRates)) +
    geom_histogram()
```

## Visualize inferred lexica example

In this game, 

```{r}
BDAparams %>% 
  #filter(trialNum %in% c(first(trialNum), last(trialNum))) %>% 
  filter(object == 'blueSquare2') %>%
  filter(word == 2) %>% 
  filter(as.integer(trialNum) %% 5 == 1) %>%
  ggplot(aes(x = value)) + 
    geom_histogram() + 
    facet_wrap(~ as.numeric(trialNum )) +
    theme_bw()

ggsave('../writing/cogsci18/figures/wordFormationExample.pdf', width = 6, height = 3)
```

```{r}
driftRates %>% group_by(as.numeric(trialNum)) %>% summarize(m = median(value))
driftRates %>% ggplot(aes(x = value)) + geom_histogram() + facet_wrap(~ as.integer(trialNum)) + theme_few()
```

## How well does behavior predict post-test?

Check labels 

```{r}
postTest_word %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.numeric(factor(text))) %>%
  ungroup() %>%
  select(word, text)

wordNumToLabelLookup <- d %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  mutate(label = text, word = as.character(as.numeric(factor(text)))) %>%
  group_by(word) %>% summarize(label = first(label)) %>%
  select(word, label)
```

```{r}
estimate_left <- function(s) {
  d0 <- density(s, from=0, to=0.05, n=1)
  return(d0$y)
}

estimate_right <- function(s) {
  d1 <- density(s, from=0.95, to=1, n=1)
  return(d1$y)
}

correctedPostTest <- postTest_word %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.character(as.numeric(factor(text)))) %>%
  group_by(text, finalRole) %>%
  filter(sum(meaning) > 0) %>%
  rename(empiricalMeaning = meaning) %>%
  select(finalRole, text, object, empiricalMeaning)
       
BDAparams %>% 
  filter(trialNum == max(trialNum)) %>%
  left_join(wordNumToLabelLookup) %>%
  group_by(text, object) %>%
  summarize(prob0 = estimate_left(value),
            prob1 = estimate_right(value),
            prediction = prob1/prob0) %>%
  filter(!is.na(text))%>%
  left_join(correctedPostTest, by = c('text', 'object')) %>% 
  ggplot(aes(y = empiricalMeaning, x = prediction)) +
    geom_point() +
    geom_smooth(method="glm", method.args = list(family = "binomial"), se = F) +
    xlab('P(meaning = 1) / P(meaning = 0)') +
    ylab('true meaning') +
    theme_bw()

ggsave('postTestPrediction.pdf', width = 3, height = 2)
```

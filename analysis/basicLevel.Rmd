---
title: "Basic-level emergence"
output:
  pdf_document: default
  html_notebook: default
  html_document: 
    smart: false
    
---

# Import libraries

```{r results="hide"}
library(tidyverse)
library(ggthemes)
library(lme4)
```

# Import data  

Import, filter out nonConvergers, pull in condition information

```{r results="hide"}
raw_clicks = read_delim('../data/experiment1/clickedObj/allClicks.csv', '\t')
raw_drops = read_delim('../data/experiment1/drop/allDrops.csv', '\t')
incompletes <- (raw_clicks %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  filter(n < 90))$gameid

masterWordIDLookup <- read_delim('../data/experiment1/postTest_word/allWordPostTest.csv', '\t') %>%
  group_by(gameid) %>%
  mutate(wordID = paste0('word', as.numeric(factor(target)))) %>%
  rename(text = target) %>%
  select(gameid, text, wordID) %>%
  distinct()

masterGameIDLookup <- raw_clicks %>%
  mutate(id = paste0('game', as.numeric(factor(gameid)))) %>%
  select(gameid, id) %>%
  distinct()
```

Filter out incompletes & compute cumulative accuracy. We also divide into quarters to compare games that ran different amounts of trials.

```{r}
d <- raw_clicks %>%
  filter(!(gameid %in% incompletes)) %>%
  mutate(acc = ifelse(correct == 'true', 1, 0)) %>%
  group_by(gameid) %>%
  mutate(quarter = floor((trialNum - 1) / (last(trialNum)/4))) %>%
  mutate(cumAcc = cumsum(acc)) %>%
  mutate(overallAcc = last(cumAcc)/last(trialNum)) %>%
  left_join(raw_drops, by = c('gameid', 'trialNum', 'intendedName')) %>%
  select(-ends_with('y'), -ends_with('x'), -correct) %>%
  left_join(masterWordIDLookup) %>%#, by = c('gameid', 'text'))  
  left_join(masterGameIDLookup)

# Exclude people who are below 75% in final quarter
nonConvergers <- (d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75))$gameid
    
cat('excluded', length(nonConvergers), 'games that never converged')
d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75) %>%
  group_by(condition) %>%
  tally()
```

## Number games per condition

```{r}
d %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  group_by(condition) %>%
  summarize(n = length(n))
```

## Write out for BDA

Want to run these webppl models in parallel, so the input data should be in separate files, easily indexed from the command-line... 

```{r}
gameIDs = unique((d %>% filter(!(gameid %in% nonConvergers)))$id)
for(i in gameIDs) {
  toWrite = d %>% 
    ungroup() %>%
    filter(id == i) %>%
    select(-gameid, -text, -condition, -contextType, -acc, -quarter, -cumAcc, -overallAcc, -timeFromRoundStart)  
  write_csv(toWrite, path = paste0('../models/bdaInput/', i, '.csv'))
}
```

# Behavioral Results 

## Accuracy distributions by quartile of game

So we can clearly see the distributions... 

```{r}
d %>% 
  group_by(gameid, quarter) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = percentCorrect)) +
    geom_histogram(bins = 10) +
    theme_few() + 
    guides(color = FALSE) +
    facet_wrap(~ quarter) 
```

We see a bimodal distribution where some people never converge (we'll exclude these for lexicon analyses).

## Overall accuracy over time

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = trialNum, y = percentCorrect)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("accuracy") +
    ylim(0,1)
```

## *Individual* cumulative accuracy curves over time

Here we see very clearly the different pairs separate out (some never converge)

```{r}
ggplot(d, aes(x = trialNum, y = cumAcc, group = gameid)) +
  geom_line() +
  theme_few() + 
  guides(color = FALSE) +
  ylab("cumulative accuracy")
```

## Accuracy by condition

```{r}
d %>% 
  group_by(condition, quarter) %>%
  summarize(percentCorrect = mean(acc), se = sd(acc)/sqrt(length(acc))) %>%
  ungroup() %>%
  mutate(condition = ifelse(condition == 'intermediateOnly', 'pure intermediate',
                            ifelse(condition == 'mixedLower', 'mixed', 'pure subordinate'))) %>%
  ggplot(aes(x = quarter, y = percentCorrect, color = condition)) +
    geom_line() +
    geom_errorbar(aes(ymin = percentCorrect - se, ymax = percentCorrect + se), width = 0.01) +
    theme_few() +
    #geom_smooth(method = 'loess') +
    scale_color_colorblind() +
    theme(    
      legend.position="top"
    ) +
    ylim(0,1)

ggsave('../writing/cogsci18/figures/accuracyByCondition.pdf', width = 5, height = 4)
```


The overall increase is significant... 

```{r}
summary(glmer(acc ~ condition + trialNum + (1 + trialNum | gameid), family = 'binomial', data = d))
```

## Accuracy by contextType

Within the mixed condition, you might expect slower improvement in sub trials?

```{r}
d %>% 
  filter(condition == 'mixedLower') %>%
  group_by(gameid, contextType, quarter) %>%
  summarize(meanAcc = mean(acc)) %>%
  group_by(contextType, quarter) %>%
  summarize(meanAcc = mean(meanAcc)) %>%
  ggplot(aes(x = quarter, y = meanAcc, color = contextType)) +
    geom_line() +
    theme_few() 

d %>% 
  filter(condition == 'mixedLower') %>%
  group_by(gameid, contextType, quarter) %>%
  summarize(meanAcc = mean(acc)) %>%
  spread(contextType, value = meanAcc) %>%
  mutate(diff = basic - sub) %>%
  group_by(quarter) %>%
  summarize(meanDiff = mean(diff), se = sd(diff)/sqrt(length(diff))) %>%
  ggplot(aes(x = quarter, y = meanDiff)) +
    geom_line() +
    geom_errorbar(aes(ymax = meanDiff + se, ymin = meanDiff - se), width = 0) +
    theme_few() + 
    theme(aspect.ratio = 1) +
    ylim(0,0.2) +
    ylab("mean accuracy difference (intermediate - sub)") +
    ggtitle("accuracy gap between trial types in mixed condition")
```

## Reaction times

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(RT = mean(timeFromRoundStart)) %>%
  ggplot(aes(x = trialNum, y = RT/1000)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("reaction time (seconds)")
```

```{r}
summary(lmer(timeFromRoundStart ~ trialNum + (1 | gameid),  data = d))
```

# Post-test results

```{r}
postTest_word = read_delim('../data/experiment1/postTest_word/allWordPostTest.csv', '\t') %>%
  mutate_each(funs(ifelse(. == "true", 1, 0)), 
              -iterationName, -gameid, -time, -target, -finalRole, -eventType) %>%
  gather(object, meaning, blueSquare1:stripedCircle2) %>%
  mutate(blue = grepl('blue', object),
         red = grepl('red', object),
         striped = grepl('striped', object),
         spotted = grepl('spotted', object),
         circle = grepl("Circle", object),
         square = grepl("Square", object)) %>%
  right_join(d %>% filter(!(gameid %in% nonConvergers))) %>%
  select(iterationName:square, condition) %>%
  group_by_at(vars(-condition)) %>%
  summarize(condition = first(condition)) %>%
  rename(text = target) %>%
  left_join(masterWordIDLookup) %>%
  left_join(masterGameIDLookup) 
length(unique(postTest_word$gameid))
```

## Consistency across two post-tests?

Read in individually because headers are all unique 
```{r results="hide"}
file_list <- list.files('../data/experiment1/postTest_object/')
postTest_obj = data.frame()
for(file in file_list) {
  result <- read_delim(file = paste0('../data/experiment1/postTest_object/', file), delim = '\t') %>%
    gather(word, meaning, -iterationName, -gameid, -time, -target, -finalRole, -eventType)
  postTest_obj = rbind(postTest_obj, result)
}
```

Combine post-tests; take intersection of meanings as the best estimate of true meaning (more conservative)

```{r}
postTest <- postTest_obj %>% 
  mutate(meaning = ifelse(meaning == 'true', 1, 0)) %>%
  filter(!(gameid %in% nonConvergers)) %>%
  rename(object = target, text = word, objectToWordMeaning = meaning) %>%
  left_join(masterWordIDLookup) %>%
  inner_join(postTest_word %>% rename(wordToObjectMeaning = meaning), by = c('gameid', 'object', 'finalRole', 'wordID')) %>%
  select(-ends_with('.x'), -ends_with('.y')) %>%
  mutate(internalConsistency = objectToWordMeaning == wordToObjectMeaning) %>%
  mutate(meaning = objectToWordMeaning & wordToObjectMeaning)
```

Look at internal consistency

```{r}
cat('have both post-test measures for', 
    length(unique(paste0(postTest$gameid, postTest$finalRole))),
    'participants')

cat('average internal consistency is ', mean((postTest %>%
  group_by(gameid, finalRole) %>%
  summarize(pctConsistent = mean(internalConsistency)) %>% 
  ungroup())$pctConsistent))

postTest %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(pctConsistent = mean(internalConsistency)) %>%
  ggplot(aes(x = pctConsistent)) +
    geom_histogram(bins = 35) +
    theme_few() +
    xlab('% of post-test responses that match in both directions')
```

## Consistency across partners

How often do players align on meanings?

We look at total overlap of matrix (i.e. how many cells differ). Compare the different measurements of meanings. 

```{r}
mismatches <- postTest %>%
  select(-blue, -red, -striped, -spotted, -circle, -square, -internalConsistency) %>%
  gather(meaningType, value, meaning, objectToWordMeaning, wordToObjectMeaning) %>%
  spread(finalRole, value) %>%
  group_by(gameid, object, wordID, condition, meaningType) %>%
  summarize(match = listener == speaker) %>%
  group_by(gameid, condition, meaningType) %>%
  summarize(numMismatches = 128-sum(match))

missingPostTests <- unique((mismatches %>% filter(is.na(numMismatches)))$gameid)
cat('have both post-test measures for', 
    length(unique((mismatches %>% filter(!(gameid %in% missingPostTests)))$gameid)),
    'pairs')

ggplot(mismatches, aes(x = numMismatches)) +
    geom_histogram(binwidth = 1) +
    #geom_vline(aes(xintercept = mean(numMatching))) +
    #xlim(-0.1,1.1) + 
    theme_few() +
    xlab('# mismatches') +
  facet_wrap(meaningType ~ condition)
```

But note that pairs that didn't technically align that well on the post-test could still perform pretty well if one partner simply has a stricter meaning than the other but the difference is never relevant.

## Do pairs with more similar lexica perform better?

TODO: reintroduce the participants who were filtered out for these main analyses

## Any violations of contrast, or things that are described by more than one word?

Basically, only this team?

```{r}
'0888-836cf6dd-4836-4d3e-bc34-2ad06f1a5352'
```

## Coverage in shared lexicon? 

This is pretty conservative, since it uses the 'intersection' metric of internal consistency: a word is only in a particular player's lexicon if they marked it in both directions, hence we're probably under-estimating their vocab. If we've underestimated both peoples' vocabs, we've also underestimated their overlap, which is probably dragging these down. Still, we get a median coverage of 7 words... 

```{r}
 coverageDF <- postTest %>%
  select(-blue, -red, -striped, -spotted, -circle, -square, -internalConsistency) %>%
  filter(!(gameid %in% missingPostTests)) %>%
  gather(meaningType, value, meaning, objectToWordMeaning, wordToObjectMeaning) %>%
  spread(finalRole, value) %>%
  group_by(gameid, object, wordID, condition, meaningType) %>%
  summarize(match = listener & speaker) %>%
  filter(meaningType == 'meaning') %>%
  group_by(gameid, object, condition) %>%
  summarize(numWord = sum(match)) %>%
  group_by(gameid, condition) %>%
  summarize(numObjectsWithWords = sum(numWord >= 1))

coverageDF %>% group_by(condition) %>% summarize(m = median(numObjectsWithWords))

ggplot(coverageDF, aes(x = condition, y = numObjectsWithWords)) +
    geom_violin() +
    ylab('# objects with shared words') +
    theme_few()
```

## Result 1: Vocab size by condition.

```{r}
lexiconSize <- postTest %>%
  group_by(gameid, finalRole, wordID, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  tally() %>%
  group_by(condition, gameid) %>%
  summarize(vocabSize = median(n, na.rm = T)) 

lexiconSize %>%
  group_by(condition) %>%
  summarize(se = sd(vocabSize)/sqrt(length(vocabSize)), vocabSize = mean(vocabSize)) %>%
  arrange(vocabSize) %>%
  mutate(condition = factor(condition, levels = unique(condition))) %>%
  ggplot(aes(x = condition, y = vocabSize)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = vocabSize + se, ymin = vocabSize - se), width = 0) +
    theme_few() +
    ylab("median vocabulary size")

ggsave('../writing/cogsci18/figures/lexiconSize.pdf', height = 3.5, width = 5)
```

```{r}
summary(lm(vocabSize ~ condition, data = lexiconSize))
```

## How many abstract vs. specific terms overall?

```{r}
postTest %>% 
  group_by(gameid, finalRole, wordID, condition) %>%
  summarize(numMeanings = sum(meaning)) %>%
  group_by(condition, numMeanings) %>%
  tally() %>%
  mutate(pct = n/sum(n)) %>%
  ggplot(aes(x = numMeanings, y = pct)) +
    geom_bar(stat = 'identity') +
    #geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0) +
    facet_wrap(~ condition) +
    theme_few() +
    xlab("# objects words refer to")

  #summarize(numAbstract = sum(abstract), numSpecific = sum(specific)) %>%
  # group_by(condition) %>%
  # summarize(numAbstract_m = mean(numAbstract), numSpecific_m = mean(numSpecific),
  #           numAbstract_se = sd(numAbstract)/sqrt(length(numAbstract)), 
  #           numSpecific_se = sd(numSpecific)/sqrt(length(numSpecific))) %>%
  # gather(metric, value, numAbstract_m:numSpecific_se) %>%
  # separate(metric, c('wordType', 'measure')) %>%
  # spread(measure, value) %>%
  # mutate(wordType = ifelse(wordType == 'numAbstract', '# abstract', '# specific')) %>%

ggsave('../writing/cogsci18/figures/lexiconContent.pdf', width = 4, height = 2)
```

## Proportion of specific & abstract within single lexicon?

```{r}
postTest %>% 
  group_by(gameid, finalRole, condition, wordID) %>%
  filter(meaning == 1) %>%
  summarize(specific = sum(meaning) == 1,
            abstract = sum(meaning) > 1) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSpecific = sum(specific),
            numAbstract = sum(abstract)) %>%
  ggplot(aes(x = numSpecific, y = numAbstract)) +#, color = numSub > 0 & numBasic > 0)) +
    geom_jitter(width = .3, height = .3, size = 2)  +
    facet_grid(~ condition) +
    theme_few() +
    xlab("# specific meanings") +
    ylab("# abstract meanings") +
    theme(aspect.ratio=1) 
  #guides(color=FALSE)
  
ggsave("../writing/cogsci18/figures/fullLexiconReport.pdf", width = 4, height =2)
```

What is modal response in each condition?

```{r}
postTest %>% 
  group_by(gameid, finalRole, wordID) %>%
  filter(meaning == 1) %>%
  summarize(subordinate = sum(meaning) == 1,
            basic = (sum(meaning) == 2 & 
                       (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
  group_by(gameid, finalRole) %>%
  summarize(numSub = sum(subordinate),
            numBasic = sum(basic)) %>%
  left_join(d) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>%
  group_by(condition, numSub, numBasic) %>%
  tally() %>%
  group_by(condition) %>%
  mutate(pct = n/sum(n)) %>%
  select(-n) %>%
  filter(pct == max(pct))
```

## Entropy of specific vs. abstract distribution within lexicon

How many objects does each label correspond to (i.e. how many meanings at sub-level vs. basic-level vs. super-level)

```{r}
postTest %>%
  group_by(gameid, finalRole, condition) %>%
  filter(sum(meaning) < 16) %>%
  group_by(gameid, finalRole, text, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  mutate(numMeanings = length(numObjects), abstract = numObjects > 1) %>%
  group_by(gameid, finalRole, abstract, condition) %>%
  summarize(pct = n()/mean(numMeanings)) %>%
  #ungroup() %>%
  group_by(condition) %>%
  complete(gameid, finalRole, abstract, fill = list(pct = 0)) %>%
  spread(abstract, pct) %>%
  mutate(entropy = -`TRUE`*log(`TRUE`) -`FALSE`*log(`FALSE`),
         entropy = ifelse(is.na(entropy), 0, entropy)) %>%
  group_by(condition) %>%
  summarize(m = mean(entropy), se = sd(entropy)/sqrt(length(entropy))) %>%
  ggplot(aes(x = condition, y = m)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0) +
    #facet_wrap(~ condition) +
    theme_few()
```

# Some questions

1. Is this data 'valid' & reliable? Does it allow us to systematically examine the real lexica that are formed? Or is it 'too hard' for many turkers, according to some criterion, so that we're mostly just seeing noise? Collect more data to estimate this better? Redesign task (e.g. only draw targets from half of heirarchy; make it much longer?)

2. Do they use words appropriately (i.e. not just to literally mean what they say it means but also in context where it's useful)?

3. What is confusion matrix of objects for diff. words? May see clustering within categories if participants haven't yet aligned on 

4. What is sequence of formation (first occurrence: basic-level, subordinate?)

5. Pragmatic effects (e.g. words on subordinate trials being extended to basic-level)

6. Do people align better when the play longer?

7. What stats to use for coexistence of basic- and sub-?

# Model analysis

Import top-level hierarchical lexical posteriors for each quarter

```{r}
library(rjson)

softplus = function(x) {
  return(log(1 + exp(x)))
}

words = paste0('word', seq(1:16))
objects = as.character(c('blueSquare1', 'blueSquare2', 'redSquare1', 'redSquare2',              
            'spottedCircle1', 'spottedCircle2', 'stripedCircle1', 'stripedCircle2'))

posteriors <- data.frame()
for(i in c(gameIDs)) {
  base <- expand.grid(object=objects, wordID=words,stringsAsFactors=T)
  result <- fromJSON(file = paste0("../models/", i, ".json"))
  speakerDF <- data.frame(model_4_mu = result$speakerHyp4mu$data, model_4_sigma = softplus(result$speakerHyp4sigma$data),
                          model_3_mu = result$speakerHyp3mu$data, model_3_sigma = softplus(result$speakerHyp3sigma$data),
                          model_2_mu = result$speakerHyp2mu$data, model_2_sigma = softplus(result$speakerHyp2sigma$data),
                          model_1_mu = result$speakerHyp1mu$data, model_1_sigma = softplus(result$speakerHyp1sigma$data)) %>%
    mutate(finalRole = 'speaker')
  listenerDF <- data.frame(model_4_mu = result$listenerHyp4mu$data, model_4_sigma = softplus(result$listenerHyp4sigma$data),
                           model_3_mu = result$listenerHyp3mu$data, model_3_sigma = softplus(result$listenerHyp3sigma$data),
                           model_2_mu = result$listenerHyp2mu$data, model_2_sigma = softplus(result$listenerHyp2sigma$data),
                           model_1_mu = result$listenerHyp1mu$data, model_1_sigma = softplus(result$listenerHyp1sigma$data)) %>%
    mutate(finalRole = 'listener')
  gameDF <- (cbind(rbind(base, base), rbind(speakerDF, listenerDF)) %>% mutate(id = i))
  posteriors <- rbind(posteriors, gameDF)
}

posteriors <- posteriors %>%   
  gather(key, value, -id, -object, -wordID, -finalRole) %>%
  separate(key, into = c('source', 'quarter', 'param')) %>%
  spread(param, value)

```

Construct ROC curves showing performance of each quarter's model:

```{r}
rocOutput = data.frame()
for(qt in seq(1: 4)) {
  qtrData = posteriors %>% 
    inner_join(postTest) %>%
    filter(quarter == qt) %>%
    
    mutate(finalPrediction = 1 - pnorm(0, mean = mu, sd = sigma))
  logisticRegression = glmer(meaning ~ finalPrediction + (1 | id), family = 'binomial', data = qtrData)
  analysis <- roc(qtrData$meaning, predict(logisticRegression))
  rocOutput <- rbind(rocOutput, (data.frame(x = 1 - analysis$specificities, y = analysis$sensitivities, q = qt)))
}
```

```{r}
ggplot(rocOutput %>% rename(quarter = q), aes(x = x, y=y, group =quarter, color = quarter)) +
    geom_line(size = 2) + 
    theme_few() +
    ylab('true positive') +
    xlab('false positive') +
    theme(aspect.ratio = 1) +
    geom_abline(aes(slope = 1, intercept = 0))+
    theme(legend.position="top")

ggsave("../writing/cogsci18/figures/modelPerformance.pdf", height = 5, width = 4)
```

### Measure accuracy?

```{r}
posteriors %>% 
  inner_join(postTest) %>%
  filter(!(gameid %in% missingPostTests)) %>%
  mutate(finalPrediction = 1 - pnorm(0, mean = model_qt1mu, sd = model_qt1sigma)) %>%
  mutate(binaryPrediction = finalPrediction > 0.585) %>%
  mutate(accuratePrediction = binaryPrediction == meaning) %>%
  group_by(condition) %>%
  summarize(m = mean(accuratePrediction, rm.na = F))
```

```{r}
posteriors %>% 
  inner_join(postTest) %>%
  filter(!(gameid %in% missingPostTests)) %>%
  mutate(meaning = ifelse(meaning == TRUE, 1, 0)) %>%
  mutate(finalPrediction = 1 - pnorm(0, mean = model_qt4mu, sd = log(1 + exp(model_qt4sigma)))) %>%
  ggplot(aes(x = finalPrediction, y = meaning, color = condition)) +
    geom_smooth(method="glm", method.args = list(family = "binomial"), se = F)+
      geom_point(alpha = 0.1, size = 2) +
      
  theme_few()
```

Examine variance across quarters
```{r}
varianceDF <- data.frame()
for(i in c(gameIDs)) {
  result <- fromJSON(file = paste0("../models/", i, ".json"))
  gameDF = data.frame(speaker_qt1_mu = result$speakerVar1mu$data, speaker_qt1_sigma = result$speakerVar1sigma$data, 
                      speaker_qt2_mu = result$speakerVar2mu$data, speaker_qt2_sigma = result$speakerVar2sigma$data, 
                      speaker_qt3_mu = result$speakerVar3mu$data, speaker_qt3_sigma = result$speakerVar3sigma$data, 
                      speaker_qt4_mu = result$speakerVar4mu$data, speaker_qt4_sigma = result$speakerVar4sigma$data, 
                      listener_qt1_mu = result$listenerVar1mu$data, listener_qt1_sigma = result$listenerVar1sigma$data, 
                      listener_qt2_mu = result$listenerVar2mu$data, listener_qt2_sigma = result$listenerVar2sigma$data, 
                      listener_qt3_mu = result$listenerVar3mu$data, listener_qt3_sigma = result$listenerVar3sigma$data, 
                      listener_qt4_mu = result$listenerVar4mu$data, listener_qt4_sigma = result$listenerVar4sigma$data) 
  varianceDF = rbind(varianceDF, gameDF %>% mutate(gameid = i))
}
```

```{r}
varianceDF %>% 
  gather(key, value, -gameid) %>%
  separate(key, into = c('finalRole', 'quarter', 'param')) %>%
  spread(param, value) %>%
  mutate(sigma = log(1+exp(sigma))) %>%
  group_by(quarter) %>%
  summarize(m = mean(mu), s = mean(sigma))
```

## Visualize inferred lexica example

In this game, 

```{r}
BDAparams %>% 
  #filter(trialNum %in% c(first(trialNum), last(trialNum))) %>% 
  filter(object == 'blueSquare2') %>%
  filter(word == 2) %>% 
  filter(as.integer(trialNum) %% 5 == 1) %>%
  ggplot(aes(x = value)) + 
    geom_histogram() + 
    facet_wrap(~ as.numeric(trialNum )) +
    theme_bw()

ggsave('../writing/cogsci18/figures/wordFormationExample.pdf', width = 6, height = 3)
```

```{r}
driftRates %>% group_by(as.numeric(trialNum)) %>% summarize(m = median(value))
driftRates %>% ggplot(aes(x = value)) + geom_histogram() + facet_wrap(~ as.integer(trialNum)) + theme_few()
```

## How well does behavior predict post-test?

Check labels 

```{r}
postTest_word %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.numeric(factor(text))) %>%
  ungroup() %>%
  select(word, text)

wordNumToLabelLookup <- d %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  mutate(label = text, word = as.character(as.numeric(factor(text)))) %>%
  group_by(word) %>% summarize(label = first(label)) %>%
  select(word, label)
```

```{r}
estimate_left <- function(s) {
  d0 <- density(s, from=0, to=0.05, n=1)
  return(d0$y)
}

estimate_right <- function(s) {
  d1 <- density(s, from=0.95, to=1, n=1)
  return(d1$y)
}

correctedPostTest <- postTest_word %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.character(as.numeric(factor(text)))) %>%
  group_by(text, finalRole) %>%
  filter(sum(meaning) > 0) %>%
  rename(empiricalMeaning = meaning) %>%
  select(finalRole, text, object, empiricalMeaning)
       
BDAparams %>% 
  filter(trialNum == max(trialNum)) %>%
  left_join(wordNumToLabelLookup) %>%
  group_by(text, object) %>%
  summarize(prob0 = estimate_left(value),
            prob1 = estimate_right(value),
            prediction = prob1/prob0) %>%
  filter(!is.na(text))%>%
  left_join(correctedPostTest, by = c('text', 'object')) %>% 
  ggplot(aes(y = empiricalMeaning, x = prediction)) +
    geom_point() +
    geom_smooth(method="glm", method.args = list(family = "binomial"), se = F) +
    xlab('P(meaning = 1) / P(meaning = 0)') +
    ylab('true meaning') +
    theme_bw()

ggsave('postTestPrediction.pdf', width = 3, height = 2)
```

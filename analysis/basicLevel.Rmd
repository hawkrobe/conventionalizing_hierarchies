---
title: "Basic-level emergence"
output:
  pdf_document: default
  html_notebook: default
  html_document: 
    smart: false
    
---

# Import libraries

```{r results="hide"}
library(tidyverse)
library(ggthemes)
library(lme4)
```

# Import data  

```{r results="hide"}
raw_clicks = read_delim('../data/pilot1/clickedObj/clickedObjData.csv', '\t')
raw_drops = read_delim('../data/pilot1/drop/dropData.csv', '\t')
incompletes <- (raw_clicks %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  filter(n < 60))$gameid
```

Filter out incompletes & compute cumulative accuracy. We also divide into quarters to compare games that ran different amounts of trials.

```{r}
d <- raw_clicks %>%
  filter(!(gameid %in% incompletes)) %>%
  mutate(acc = ifelse(correct == 'true', 1, 0)) %>%
  group_by(gameid) %>%
  mutate(condition = case_when(condition == 'over' ~ 'sub-majority',
                               condition == 'under' ~ 'super-majority',
                               condition == 'basic' ~ 'basic-majority',
                               condition == 'uniform' ~ 'uniform')) %>%
  mutate(numRounds = last(trialNum)) %>%
  mutate(quarter = floor((trialNum - 1) / (last(trialNum)/4))) %>%
  mutate(cumAcc = cumsum(acc)) %>%
  mutate(overallAcc = last(cumAcc)/last(trialNum)) %>%
  ungroup() %>%
  left_join(raw_drops, by = c('gameid', 'trialNum', 'intendedName')) %>%
  select(-ends_with('y'), -ends_with('x'), -correct)

# Exclude people who are below 50% in final quarter
nonConvergers <- (d %>% 
  filter(quarter == 3) %>%
  group_by(gameid) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75))$gameid
    
cat('excluded', length(nonConvergers), 'games that never converged')
```

## Number games per condition

```{r}
d %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  group_by(condition) %>%
  summarize(n = length(n))
```

## Write out for BDA

```{r}
d %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  select(-condition, -contextType, -acc, -numRounds, -quarter, -cumAcc, -overallAcc, -timeFromRoundStart) %>% 
  mutate(label = text, text = paste0('word', as.numeric(factor(text)))) %>% 
  write_csv(path = '../models/bdaInput/singleGame.csv') 
```

# Behavioral Results 

## Accuracy distributions by quartile of game

```{r}
d %>% 
  #filter(numRounds == 90) %>%
  group_by(gameid, quarter) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = percentCorrect)) +
    geom_histogram(bins = 10) +
    theme_few() + 
    guides(color = FALSE) +
    facet_wrap(~ quarter) 
```

We see a bimodal distribution where some people never converge (exclude for subsequent analyses).

## Overall accuracy over time

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = trialNum, y = percentCorrect)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("accuracy") +
    ylim(0,1.15)
```

This overall increase is significant... 

```{r}
summary(glmer(acc ~ trialNum + (1 + trialNum | gameid), family = 'binomial', data = d))
```

## *Individual* accuracy curves over time

Here we see very clearly the different pairs separate out (some never converge)

```{r}
ggplot(d, aes(x = trialNum, y = cumAcc, group = gameid)) +
  geom_line() +
  theme_few() + 
  guides(color = FALSE) +
  ylab("cumulative accuracy")
```

## Accuracy by condition

```{r}
d %>% 
  group_by(condition, trialNum) %>%
  summarize(meanAcc = mean(cumAcc), se = sd(cumAcc)/sqrt(length(cumAcc))) %>%
  ggplot(aes(x = trialNum, y = meanAcc, color = condition)) +
    geom_line() +
#    geom_errorbar(aes(ymin = meanAcc - se, ymax = meanAcc + se)) +
    theme_few() +
    scale_color_colorblind()
```

## Accuracy by contextType

```{r}
d %>% 
  group_by(contextType, quarter) %>%
  summarize(meanAcc = mean(acc), se = sd(acc)/sqrt(length(acc))) %>%
  ggplot(aes(x = quarter, y = meanAcc)) +
    geom_line() +
    theme_few() + 
  facet_wrap(~ contextType)
```

One hypothesis is that in conditions where you don't see very many sub contexts, performance on those contexts should stay really bad or go down as you specialize for the other trials (especially in the 'under' condition). In the uniform condition, though, you should see it still going up because people need all the words. 

```{r}
d %>% 
  group_by(condition, contextType, quarter) %>%
  summarize(meanAcc = mean(acc), se = sd(acc)/sqrt(length(acc))) %>%
  ggplot(aes(x = quarter, y = meanAcc)) +
    geom_line() +
    theme_few() + 
    facet_grid(condition ~ contextType) +
    theme(aspect.ratio = 1)
```

## Reaction times

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(RT = mean(timeFromRoundStart)) %>%
  ggplot(aes(x = trialNum, y = RT/1000)) +
    geom_point() +
    theme_few() + 
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("reaction time (seconds)")
```

```{r}
summary(lmer(timeFromRoundStart ~ trialNum + (1 | gameid),  data = d))
```

# Post-test results

Import, filter out nonConvergers, pull in condition information

```{r results="hide"}
postTest <- read_delim('../data/pilot1/postTestData/postTestData.csv', '\t') %>%
  mutate_each(funs(ifelse(. == "true", 1, 0)), 
              -iterationName, -gameid, -time, -label, -finalRole, -eventType) %>%
  gather(object, meaning, blueSquare1:stripedCircle2) %>%
  mutate(blue = grepl('blue', object),
         red = grepl('red', object),
         striped = grepl('striped', object),
         spotted = grepl('spotted', object),
         circle = grepl("Circle", object),
         square = grepl("Square", object)) %>%
  filter(!(gameid %in% nonConvergers)) %>%
  right_join(d %>% filter(!(gameid %in% nonConvergers))) %>%
  select(iterationName:square, condition) %>%
  group_by_at(vars(-condition)) %>%
  summarize(condition = first(condition))
```

## Distribution of meanings 

How many objects does each label correspond to (i.e. how many meanings at sub-level vs. basic-level vs. super-level)

```{r}
postTest %>%
  group_by(gameid, finalRole, label, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  ggplot(aes(x = numObjects, y = ..count..)) +
    geom_histogram() +
    facet_wrap(~ condition) +
    theme_few()
```

## How often do players align on meanings?

On average, pairs exactly match on about 50% of the meanings they mark... 

```{r}
postTest %>%
  select(-time) %>%
  spread(finalRole, meaning) %>%
  filter(listener > 0 | speaker > 0) %>%
  group_by(gameid, label) %>%
  summarize(match = all(listener == speaker)) %>%
  group_by(gameid) %>%
  summarize(numMatching = sum(match) / length(match)) %>%
  left_join(d) %>%
  ggplot(aes(x = numMatching, y = ..density..)) +
    geom_histogram(binwidth = .2) +
    geom_vline(aes(xintercept = mean(numMatching))) +
    xlim(0,1) + 
    theme_few() +
    xlab('% matching')
  # facet_wrap(~ condition)
```

Unsurprisingly, pairs that aligned on meanings better performed better... 

```{r}
postTest %>%
  select(-time) %>%
  spread(finalRole, meaning) %>%
  filter(listener > 0 | speaker > 0) %>%
  group_by(gameid, label) %>%
  summarize(match = all(listener == speaker)) %>%
  group_by(gameid) %>%
  summarize(numMatching = sum(match) / 16) %>%
  left_join(d) %>%
  group_by(gameid) %>%
  summarize(numMatching = mean(numMatching), 
            overallAcc = mean(overallAcc)) %>%
  ggplot(aes(x = numMatching, y = overallAcc)) +
    geom_point() +
    theme_few() +
    geom_smooth(method = 'lm')
```

But note that pairs that didn't technically align that well could still perform pretty well if one partner simply has a stricter meaning than the other but the difference is never relevant. 

When people fail to perfectly align, do they do so in a predictable way? (e.g. one meaning a subset of the other?)

```{r}
```

## Vocab size by condition... 

```{r}
postTest %>%
  group_by(gameid, finalRole, label, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  tally() %>%
  group_by(condition, gameid) %>%
  summarize(vocabSize = mean(n, na.rm = T)) %>%
  group_by(condition) %>%
  summarize(se = sd(vocabSize)/sqrt(length(vocabSize)), vocabSize = mean(vocabSize)) %>%
  arrange(vocabSize) %>%
  mutate(condition = factor(condition, levels = unique(condition))) %>%
  ggplot(aes(x = condition, y = vocabSize)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = vocabSize + se, ymin = vocabSize - se), width = 0) +
    theme_few()

ggsave('../writing/cogsci18/lexiconSize.pdf', height = 3.5, width = 5)
```

## How many abstract vs. specific terms overall?

```{r}
postTest %>% 
  group_by(gameid, finalRole, label, condition) %>%
  summarize(specific = sum(meaning) == 1, abstract = sum(meaning) > 1) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numAbstract = sum(abstract), numSpecific = sum(specific)) %>%
  group_by(condition) %>%
  summarize(numAbstract_m = mean(numAbstract), numSpecific_m = mean(numSpecific),
            numAbstract_se = sd(numAbstract)/sqrt(length(numAbstract)), 
            numSpecific_se = sd(numSpecific)/sqrt(length(numSpecific))) %>%
  gather(metric, value, numAbstract_m:numSpecific_se) %>%
  separate(metric, c('wordType', 'measure')) %>%
  spread(measure, value) %>%
  ggplot(aes(x = wordType, y = m)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(aes(ymax = m + se, ymin = m - se), width = 0) +
    facet_wrap(~ condition) +
    theme_few()

ggsave('../writing/cogsci18/lexiconContent.pdf', width = 5, height = 5)
```

## Proportion of specific & abstract within single lexicon?

```{r}
postTest %>% 
  group_by(gameid, finalRole, label) %>%
  filter(meaning == 1) %>%
  summarize(subordinate = sum(meaning) == 1,
            basic = (sum(meaning) == 2 & 
                       (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
  group_by(gameid, finalRole) %>%
  summarize(numSub = sum(subordinate),
            numBasic = sum(basic)) %>%
  left_join(d) %>%
  filter(condition %in% c('uniform', 'sub-majority', 'basic-majority')) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>%
  ggplot(aes(x = numSub, y = numBasic)) +#, color = numSub > 0 & numBasic > 0)) +
    geom_jitter(width = .2, height = .2, size = 3)  +
  facet_grid(~ condition) +
  theme_few() +
  xlab("# subordinate meanings") +
  ylab("# basic meanings") +
  theme(aspect.ratio=1) 
  #guides(color=FALSE)
  
ggsave("../../writing/evolang18/result.png")
# postTest %>% 
#   group_by(gameid, finalRole, label) %>%
#   summarize(subordinate = sum(meaning) == 1,
#             basic = 
```

Next, can just look at means...

```{r}
postTest %>% 
   group_by(gameid, finalRole, label) %>%
   filter(meaning == 1) %>%
   summarize(subordinate = sum(meaning) == 1,
             basic = (sum(meaning) == 2 & 
                          (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
   group_by(gameid, finalRole) %>%
   summarize(numSub = sum(subordinate),
             numBasic = sum(basic)) %>%
   left_join(d) %>%
   filter(condition %in% c('uniform', 'sub-majority', 'basic-majority')) %>%
   filter(overallAcc > .5) %>%
   group_by(gameid, finalRole, condition) %>%
   summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>% 
   group_by(condition) %>% 
   summarize(sub = mean(numSub), # suber = sd(numSub)/sqrt(length(numSub)), 
             basic = mean(numBasic)) %>%#, basicer = sd(numBasic)/sqrt(length(numBasic))) %>% 
   ggplot(aes(x = sub, y = basic)) +
    geom_point(size = 3) +
    #geom_text(aes(label = condition), nudge_y = -.1, nudge_x = .2, ) +
    theme_few(20) +
    ylim(0, 4) +
    xlim(0, 10) +
    xlab("mean # subordinate-level labels") +
    ylab("mean # basic-level labels") +
  theme(aspect.ratio = 1)
```


# Some questions

1. Is this data 'valid' & reliable? Does it allow us to systematically examine the real lexica that are formed? Or is it 'too hard' for many turkers, according to some criterion, so that we're mostly just seeing noise? Collect more data to estimate this better? Redesign task (e.g. only draw targets from half of heirarchy; make it much longer?)

2. Do they use words appropriately (i.e. not just to literally mean what they say it means but also in context where it's useful)?

3. What is confusion matrix of objects for diff. words? May see clustering within categories if participants haven't yet aligned on 

4. What is sequence of formation (first occurrence: basic-level, subordinate?)

5. Pragmatic effects (e.g. words on subordinate trials being extended to basic-level)

6. Do people align better when the play longer?

7. What stats to use for coexistence of basic- and sub-?

# Model analysis

Import lexical posteriors

```{r}
BDAparams <- read_csv('../models/bdaOutput/lexicalInferenceParams.csv') %>%
  mutate(sample = row_number()) %>%
  gather(param, value, -sample) %>%
  separate(param, into = c('word', 'object', 'trialNum'))
```

```{r}
read_csv('../models/bdaOutput/lexicalInferenceDrifts.csv') %>%
  mutate(sample = row_number())  %>%
  ggplot(aes(x = driftRates)) +
    geom_histogram()
```

## Visualize inferred lexica example

In this game, 

```{r}
BDAparams %>% 
  #filter(trialNum %in% c(first(trialNum), last(trialNum))) %>% 
  filter(object == 'blueSquare2') %>%
  filter(word == 2) %>% 
  filter(as.integer(trialNum) %% 5 == 1) %>%
  ggplot(aes(x = value)) + 
    geom_histogram() + 
    facet_wrap(~ as.numeric(trialNum )) +
    theme_bw()

ggsave('wordFormation.pdf', width = 6, height = 3)
```

```{r}
driftRates %>% group_by(as.numeric(trialNum)) %>% summarize(m = median(value))
driftRates %>% ggplot(aes(x = value)) + geom_histogram() + facet_wrap(~ as.integer(trialNum)) + theme_few()
```

## How well does behavior predict post-test?

Check labels 

```{r}
postTest %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.numeric(factor(label))) %>%
  ungroup() %>%
  select(word, label)

wordNumToLabelLookup <- d %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  mutate(label = text, word = as.character(as.numeric(factor(text)))) %>%
  group_by(word) %>% summarize(label = first(label)) %>%
  select(word, label)
```

```{r}
estimate_left <- function(s) {
  d0 <- density(s, from=0, to=0.05, n=1)
  return(d0$y)
}

estimate_right <- function(s) {
  d1 <- density(s, from=0.95, to=1, n=1)
  return(d1$y)
}

correctedPostTest <- postTest %>% 
  filter(gameid == '0234-6e789adc-7489-4d55-b032-287910407ed7') %>% 
  group_by(gameid,finalRole) %>%
  mutate(word = as.character(as.numeric(factor(label)))) %>%
  group_by(label, finalRole) %>%
  filter(sum(meaning) > 0) %>%
  rename(empiricalMeaning = meaning) %>%
  select(finalRole, label, object, empiricalMeaning)
       
BDAparams %>% 
  filter(trialNum == max(trialNum)) %>%
  left_join(wordNumToLabelLookup) %>%
  group_by(label, object) %>%
  summarize(prob0 = estimate_left(value),
            prob1 = estimate_right(value),
            prediction = prob1/prob0) %>%
  filter(!is.na(label))%>%
  left_join(correctedPostTest, by = c('label', 'object')) %>% 
  ggplot(aes(y = empiricalMeaning, x = prediction)) +
    geom_point() +
    geom_smooth(method="glm", method.args = list(family = "binomial"), se = F) +
    xlab('P(meaning = 1) / P(meaning = 0)') +
    ylab('true meaning') +
    theme_bw()

ggsave('postTestPrediction.pdf', width = 3, height = 2)
```

///fold:
var getTrajectories = function(data) {
  var keys = _.keys(data[0]);
  return reduce(function(key, memo) {
    var timeBasedKeys = map(function(i) {return key + "." + i}, _.range(data.length));
    var vals = _.map(data, key);
    return extend(_.zipObject(timeBasedKeys, vals), memo)
  }, [], keys)
};
///

// possible states of the world
var states = ['blueSquare1', 'blueSquare2', 'redSquare1', 'redSquare2',
	      'spottedCircle1', 'spottedCircle2', 'stripedCircle1', 'stripedCircle2'];
var statePrior =  Categorical({vs: states});

// possible utterances
var utterances = map(function(i) {return 'word' + i;}, _.range(1, 17));
var utterancePrior = Categorical({vs: utterances});

var meanings = [[1,0,0,0,0,0,0,0],
                [0,1,0,0,0,0,0,0],
                [0,0,1,0,0,0,0,0],
                [0,0,0,1,0,0,0,0],
                [0,0,0,0,1,0,0,0],
                [0,0,0,0,0,1,0,0],
                [0,0,0,0,0,0,1,0],
                [0,0,0,0,0,0,0,1],
                [1,1,0,0,0,0,0,0],
                [0,0,1,1,0,0,0,0],
                [0,0,0,0,1,1,0,0],
                [0,0,0,0,0,0,1,1],
                [1,1,1,1,0,0,0,0],
                [0,0,0,0,0,0,0,0],		
                [0,0,0,0,1,1,1,1]];

var sampleParams = function(i) {
  var lexDist = DiagCovGaussian({mu: zeros([utterances.length,states.length]),
				 sigma: ones([utterances.length,states.length])});
  return sample(lexDist);
  //var meaningsList = map(function(utt) {
    // var lexicalEntriesList = categorical({vs: meanings});
    // return _.zipObject(states, lexicalEntriesList);
//}, utterances);
  //return _.zipObject(utterances, meaningsList);
};

// var sampleLexicon = function(i) {
//   var meaningsList = map(function(utt) {
//     var lexicalEntriesList = repeat(states.length, function() {
//       return flip(.1) ? 0 : -Infinity;});//flip();});
//     return _.zipObject(states, lexicalEntriesList);
//   }, utterances);
//   return _.zipObject(utterances, meaningsList);
// };

var transition = function(previousParams, driftRate) {
  //  console.log('transitioning... with new stability ' + stability);
  var adjustmentDist = DiagCovGaussian({
    mu: zeros([utterances.length,states.length]),
    sigma: T.mul(ones([utterances.length,states.length]), driftRate)
  });
  return T.add(previousParams, sample(adjustmentDist));
};

// literal listener (using real-valued lexicon)
var L0 = cache(function(utt, context, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = uniformDraw(context);
    factor(lexicon[utt][state] ? 0 : -1000);
    return state;
  });
});

// pragmatic speaker
var alpha = 1;
var fixedS1 = cache(function(state, context, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var utt = uniformDraw(utterances);
    factor(alpha * refModule.getL0score(state, utt, context, lexicon));
    return utt;
  });
});

// conventional listener
var L1 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    observe(S1(state, lexicon), utt);
    return state;
  });
});

// compute lexicon posterior, taking into account some previous observations
// speakers do this by assuming data came from knowledgable listener, and vice versa
var lexiconPosterior = cache(function(originAgent, data) {
  return Infer({method: 'enumerate'}, function() {
    var lexicon = sample(lexiconPrior);
    mapData({data: data}, function(datum){
      if(originAgent === 'L') 
        observe(S1(datum.response, lexicon), datum.utt);
      else if(originAgent === 'S') 
        observe(L1(datum.utt, lexicon), datum.response);
    });
    return lexicon;
  });
});

// conventional listener (L1, marginalizing over lexicons)
var L = cache(function(utt, data) {
  return Infer({method:"enumerate"}, function(){
    var lexicon = sample(lexiconPosterior('L', data));
    var state = sample(L1(utt, lexicon));
    return state;
  });
});

// conventional speaker (S1, reasoning about expected L1 behavior across lexicons)
var S = cache(function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var utt = sample(utterancePrior);
    var listener = Infer({method: 'enumerate'}, function() {
      var lexicon = sample(lexiconPosterior('S', data));
      return sample(L1(utt, lexicon))
    });
    factor(params.alpha * listener.score(state)
           - params.beta * uttCost(utt));
    return utt;
  });
});

// var model = function() {
//   var step = function(data) {
//     if(data.length > params.numSteps) return getTrajectories(data);
//     var state = sample(statePrior);
//     var utt = sample(S(state, data));
//     var response = sample(L(utt, data));
//     var newDatum = {utt, response, intended: state, acc: state == response};
//     return step(data.concat(newDatum));
//   };
//   step([]);
// };

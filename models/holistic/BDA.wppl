// run using, e.g.:
// WEBPPL_PARAM_PATH='./bdaOutput/'; echo $WEBPPL_PARAM_PATH; webppl BDA.wppl --param-store file --param-id game1 --require ./refModule/ -- --gameid game1

// Load in experimental data to condition on then reformat
var rawData = refModule.readCSV('../bdaInput/' + argv.gameid + '.csv');
var data = refModule.reformatData(rawData);
var numSplits = 6;
var splitSize = data.length / numSplits;
console.log("Loading expt data complete..." + data.length + " data points");

var globalConfig = {
  aggregate: false,
  outputFileName : argv.gameid + 'lexicalInference'
};

var utterances = map(function(i) {return 'word' + i;}, _.range(1, 17));
var states = ['blueSquare1', 'blueSquare2', 'redSquare1', 'redSquare2',
	      'spottedCircle1', 'spottedCircle2', 'stripedCircle1', 'stripedCircle2'];

var scalarSoftplus = function(x) {
  return ad.scalar.log(ad.scalar.add(ad.scalar.exp(x), 1));
};

var tensorSoftplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var reparam = function(s, mu, sig) {
  return T.add(T.mul(s, sig), mu);
};

var lexiconGuide = function(paramName, dims) {
  return function() {
    DiagCovGaussian({
      mu: param({name: paramName + 'mu', dims: dims}),
      sigma: tensorSoftplus(param({name: paramName + 'sigma', dims: dims}))
    });
  };
};

// var varianceGuide = function(p) {
//   return function() {
//     Gaussian({
//       mu: param({name: 'VarMu'}),
//       sigma: scalarSoftplus(param({name: 'VarSigma'}))
//     });
//   };
// };

// Hierarchical model with one lexicon each split
// This takes those hierarchical lexica as input and samples a lexicon for round
// var sampleVariances = function() {
//   var dims = [utterances.length,states.length];
//   var varianceDist = Gaussian({mu: 0, sigma: .1});
//   var sigma = sample(varianceDist, {guide: varianceGuide()});
//   return T.mul(ones(dims), scalarSoftplus(sigma));
// };

var sampleHyperlexica = function(q) {
  var dims = [utterances.length,states.length];
  var lexDist = DiagCovGaussian({ mu: zeros(dims), sigma: T.mul(ones(dims),100) });
  return {
    finalSpeaker:  sample(lexDist, {guide: lexiconGuide('speakerHyp' + q, dims)}),
    finalListener: sample(lexDist, {guide: lexiconGuide('listenerHyp' + q, dims)})
  };
};

// Hierarchical model with one lexicon each split
// This takes those hierarchical lexica as input and samples a lexicon for round
// var sampleLexiconParams = function(hyperparams, datum) {
//   var i = datum.trialNum;
//   var dims = [utterances.length,states.length];
//   var lexDist = DiagCovGaussian({ mu: zeros(dims), sigma: ones(dims) });  
//   return {
//     finalSpeaker:  reparam(sample(lexDist, {guide: lexiconGuide('speaker'  + i, dims)}),
// 			   hyperparams.mu.finalSpeaker, hyperparams.sigma),
//     finalListener: reparam(sample(lexDist, {guide: lexiconGuide('listener' + i, dims)}),
// 			   hyperparams.mu.finalListener, hyperparams.sigma)
//   };
// };

// For now, we are just doing a pure statistical model -- not trying to tie
// their new lexicon mechanistically to what happened on the previous round,
// just trying to learn what it is on the basis of what they said.
var observeRound = function(params, datum) {
  // Align role swapping with final post-test measures
  var llex = datum.trialNum % 2 == 1 ? params.finalSpeaker : params.finalListener;
  var slex = datum.trialNum % 2 == 1 ? params.finalListener : params.finalSpeaker;
  
  // transition happens on raw params; must transform to [0,1] before passing as lexicon
  var speakerScore = refModule.getSpeakerScore(datum.wordID, datum.intendedName, {
    context: datum.context,
    lexicon: T.sigmoid(slex),
    utterances: utterances,
    alpha: globalStore.alpha
  });
  factor(speakerScore);

  var listenerScore = refModule.getListenerScore(datum.clickedName, datum.wordID,{
    context: datum.context,
    lexicon: T.sigmoid(llex),
    utterances: utterances,
    alpha: globalStore.alpha
  });
  factor(listenerScore);
};

var model = function() {
  // Sample hyperparams
  globalStore.alpha = 1;
  var mus = map(function(q) {return sampleHyperlexica(q);}, _.range(1,numSplits + 1));

  mapData({data: data, batchSize: 96}, function(trialDatum) {
    var currQ = Math.floor((trialDatum.trialNum - 1) / splitSize);
    var lexiconParams = mus[currQ];
    observeRound(lexiconParams, trialDatum);
  });

};
  
Optimize({model: model, steps: 10000, verbose: true,
	  optMethod: {adam: {stepSize: 0.01}}});

//refModule.bayesianErpWriter(outputERP, "./bdaOutput/" + globalConfig.outputFileName);

// run using, e.g.:
// WEBPPL_PARAM_PATH='./bdaOutput/'; echo $WEBPPL_PARAM_PATH; webppl predict.wppl --param-store file --param-id game1 --require ./refModule/ -- --gameid game1

// Load in experimental data to condition on then reformat
var rawData = refModule.readCSV('../bdaInput/' + argv.gameid + '.csv');
var data = refModule.reformatData(rawData);
var quartileSize = data.length / 4;
console.log("Loading expt data complete..." + data.length + " data points");

var globalConfig = {
  aggregate: false,
  outputFileName : 'lexicalInference'
};

var utterances = map(function(i) {return 'word' + i;}, _.range(1, 17));
var states = ['blueSquare1', 'blueSquare2', 'redSquare1', 'redSquare2',
	      'spottedCircle1', 'spottedCircle2', 'stripedCircle1', 'stripedCircle2'];

var scalarSoftplus = function(x) {
  return ad.scalar.log(ad.scalar.add(ad.scalar.exp(x), 1));
};

var tensorSoftplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var lexiconGuide = function(paramName, dims) {
  return function() {
    DiagCovGaussian({
      mu: param({name: paramName + 'mu', dims: dims}),
      sigma: T.mul(ones(dims), .001) 
    });
  };
};

var sampleHyperlexica = function(q) {
  var dims = [utterances.length,states.length];
  var lexDist = DiagCovGaussian({ mu: zeros(dims), sigma: ones(dims) });
  return {
    finalSpeaker:  sample(lexDist, {guide: lexiconGuide('speakerHyp' + q, dims)}),
    finalListener: sample(lexDist, {guide: lexiconGuide('listenerHyp' + q, dims)})
  };
};

var reparam = function(s, m, s) {
  return T.mul(T.add(s, m), s);
};

// literal listener (using real-valued lexicon)
var L0 = function(utt, context, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = uniformDraw(context);
    factor(refModule.getLexiconElement(lexicon, utt, state));
    return state;
  });
};

var S1 = function(state, context, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var utt = uniformDraw(utterances);
    factor(globalStore.alpha * L0(utt, context, lexicon).score(state));
    return utt;
  });
};

// conventional listener
var L1 = function(utt, context, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = uniformDraw(states);
    observe(S1(state, context, lexicon), utt);
    return state;
  });
};

var model = function() {
  // Sample hyperparams
  
  globalStore.alpha = 50;
  var mus = map(function(q) {return sampleHyperlexica(q);}, _.range(1,5));

  map(function(quarter) {
    map(function(utt) {
      map(function(participant) {
	var hyperparams = {
	  mu: mus[quarter][participant]
	};
	var responses = L1(utt, states, T.sigmoid(hyperparams.mu));
	var key = [quarter, utt, participant,
		   entropy(responses)].join(",");
	var newPair = _.zipObject([key], [argv.gameid]);
	console.log(newPair);
	globalStore.predictives = extend(globalStore.predictives, newPair);
      }, ['finalListener', 'finalSpeaker']);
    }, utterances);
  }, _.range(4));

  return {predictive: globalStore.predictives};
};

var outputERP = Infer({model: model, method: 'forward', samples: 1, onlyMAP: true, guide: true});
refModule.bayesianErpWriter(outputERP, "./bdaOutput/" + globalConfig.outputFileName);

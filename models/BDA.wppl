// Run using, e.g.:
// WEBPPL_PARAM_PATH='./bdaOutput/'; echo $WEBPPL_PARAM_PATH; webppl BDA.wppl --param-store file --param-id game1 --require ./refModule/ -- --gameid game1

// Load in experimental data to condition on then reformat
var rawData = refModule.readCSV('./bdaInput/' + argv.gameid + '.csv');
var data = refModule.reformatData(rawData);
var quartileSize = data.length / 4;
console.log("Loading expt data complete..." + data.length + " data points");

var globalConfig = {
  aggregate: false,
  outputFileName : argv.gameid + 'lexicalInference'
};

// Objects defined by features [squirle-ness,red-ness,blue-ness,stripe-ness,spot-ness]
var numUtterances = 16;
var numFeatures = 5;
var dims = [numUtterances, numFeatures];
var lexPrior = DiagCovGaussian({ mu: zeros(dims), sigma: T.mul(ones(dims), 10)}); 

// Some tensor helpers
var scalarSoftplus = function(x) {
  return ad.scalar.log(ad.scalar.add(ad.scalar.exp(x), 1));
};

var tensorSoftplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var reparam = function(distSample, mean, sd) {
  return T.mul(T.add(distSample, mean), sd);
};

// Hierarchical model with one lexicon each quarter
var sampleHyperlexica = function(q) {
  return {
    speaker:  sample(lexPrior, {guide: lexiconGuide('speakerCentHyp' + q, dims)}),
    listener: sample(lexPrior, {guide: lexiconGuide('listenerCentHyp' + q, dims)})
  };
};

var lexiconGuide = function(paramName, dims) {
  return function() {
    return DiagCovGaussian({
      mu: param({name: paramName + 'mu', dims: dims}),
      sigma: tensorSoftplus(param({name: paramName + 'sigma', dims: dims}))
    });
  };
};

   // var sampleVariances = function(q) {
//   var dims = [utterances.length,numFeatures];
//   var speakerVar = sample(Gaussian({mu: 0, sigma: 1}), {guide: function() {
//     return Gaussian({mu: param({name: 'speakerVar' + q + 'mu'}),
// 		     sigma: scalarSoftplus(param({name: 'speakerVar' + q + 'sigma'}))});
//   }});
//   var listenerVar = sample(Gaussian({mu: 0, sigma: 1}), {guide: function() {
//     return Gaussian({mu: param({name: 'listenerVar' + q + 'mu'}),
// 		     sigma: scalarSoftplus(param({name: 'listenerVar' + q + 'sigma'}))});
//   }});
//   return {
//     speaker:  T.mul(ones(dims), scalarSoftplus(speakerVar)),
//     listener: T.mul(ones(dims), scalarSoftplus(listenerVar))
//   };
// };

// On a particular trial, you can accomodate some adjustment from hyper if necessary
var sampleLexiconParams = function(hyperparams, datum) {
  var i = datum.trialNum;
  return {
    speaker: reparam(sample(lexPrior, {guide: lexiconGuide('speakerCent'  + i, dims)}),
		      hyperparams.speaker, 1),
    listener: reparam(sample(lexPrior, {guide: lexiconGuide('listenerCent' + i, dims)}),
		      hyperparams.listener, 1)
  };
};

// For now, we are just doing a pure statistical model -- not trying to tie
// their new lexicon mechanistically to what happened on the previous round,
// just trying to learn what it is on the basis of what they said.
var observeRound = function(params, datum) {
  // Align role swapping with final post-test measures
  var llex = datum.trialNum % 2 == 1 ? params.speaker : params.listener;
  var slex = datum.trialNum % 2 == 1 ? params.listener : params.speaker;
  
  // transition happens on raw params; must transform to [0,1] before passing as lexicon
  var speakerScore = refModule.getSpeakerScore(datum.wordID, datum.intendedName, {
    context: datum.context,
    lexicon: slex,
    alpha: globalStore.alpha
  });
  factor(speakerScore);

  var listenerScore = refModule.getListenerScore(datum.clickedName, datum.wordID, {
    context: datum.context,
    lexicon: llex,
    alpha: globalStore.alpha
  });
  factor(listenerScore);
};

var model = function() {
  // Sample hyperparams
  // globalStore.alpha = scalarSoftplus(sample(Gaussian({mu:0,sigma:10}), {
  //   guide: function() {
  //     return Gaussian({mu: param({name: 'alpha_mu'}),
  // 		       sigma: scalarSoftplus(param({name: 'alpha_sigma'}))});
  //   }
  // }));
  globalStore.alpha = 5;
  var mus = map(function(q) {return sampleHyperlexica(q);}, _.range(1,5));
  //var sigmas = map(function(q) {return sampleVariances(q);}, _.range(1,5));

  mapData({data: data}, function(trialDatum) {
    var currQ = Math.floor((trialDatum.trialNum - 1) / quartileSize);
    var hyperparams = mus[currQ];
    var lexiconParams = sampleLexiconParams(hyperparams, trialDatum);
    observeRound(lexiconParams, trialDatum);
  });

  // return refModule.reformatParams(params, data,
  // 				  globalStore.drift, globalStore.alpha);
};
  
Optimize({model: model, steps: 500, verbose: true,
	  optMethod: {adam: {stepSize: 0.01}}});

//refModule.bayesianErpWriter(outputERP, "./bdaOutput/" + globalConfig.outputFileName);

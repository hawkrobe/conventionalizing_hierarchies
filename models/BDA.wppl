// run using:
// webppl BDA.wppl --require ./refModule/

// Load in experimental data to condition on
var gameid = argv.gameid;
console.log(gameid);
var rawData = _.slice(refModule.readCSV('./bdaInput/' + gameid + '.csv'));

var softplus = function(x) {
  return ad.scalar.log(ad.scalar.add(1, ad.scalar.exp(x)));
};

// Reformat with context arrays...
var data = map(function(row) {
  return _.omit(extend(row, {
    context: [row.object1name, row.object2name, row.object3name, row.object4name]
  }), 'object1name', 'object2name', 'object3name', 'object4name');
}, rawData);
console.log("Loading expt data complete..." + data.length + " data points");
//console.log(data);

// Package into config
var globalConfig = {
  aggregate: false,
  outputFileName : 'lexicalInference'
};

// TODO: note that players actually swapped roles every round...

// For now, we are just doing a pure statistical model -- not trying to tie
// their new lexicon mechanistically to what happened on the previous round,
// just trying to learn what it is on the basis of what they said.
// Critically, for now, lexical uncertainty is from POV of analyst, not agent
var observeRound = function(previousParams, datum, drift) {
  var newParams = (_.isEmpty(previousParams) ? sampleParams() :
		   transition(previousParams, globalStore.drift));

  // transition happens on raw params; must transform to [0,1] before passing as lexicon
  var speakerScore = refModule.getSpeakerScore(datum.text, datum.intendedName,
					       datum.context, T.sigmoid(newParams),
					       {utterances: utterances, alpha: 5});
  factor(speakerScore);
  return newParams;
};

var model = function(data, previousParams) {
  var newParams = observeRound(previousParams[0], data[0]);
  return (data.length === 1 ?
	  previousParams.concat(newParams) :
	  previousParams.concat(model(_.tail(data), [newParams])));
};

// TODO: add alpha as global param
var modelPosterior = function() {
  globalStore.drift = softplus(sample(Gaussian({mu:0,sigma:0.1})));
  var modelResult = model(data, []);
  return refModule.reformatParams(modelResult, data, globalStore.drift);
  
  //return output; // {
  //  'params' : T.toScalars(T.split(lexicon, [8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8])[1])
  //   //{'params' : _.last(modelSample.lexica)['word2']};
  // };
};

//var outputERP = Infer({method: 'MCMC', samples:50000, onlyMAP:true,verbose:true, model: modelAnalysis});

//var outputERP = Infer({method: 'MCMC', verbose: true, samples: 10000, kernel: {HMC: {steps: 100, stepSize: .001}}, model: modelAnalysis});

//Optimize(modelShell, {verbose: true, steps: 10000});//_.omit(options, 'samples', 'onlyMAP'));

var outputERP = Infer({method: 'optimize', verbose: true, samples: 250, steps: 1000, optMethod: {adam: {stepSize: 0.01}}, model: modelPosterior});
//var outputERP = Infer({method: 'optimize', verbose: true, steps: 10000, samples: 250, model: modelAnalysis})//, particles: 10000, rejuvSteps: 100, onlyMAP: true, model: modelAnalysis});
//console.log(outputERP);
refModule.bayesianErpWriter(outputERP, "./bdaOutput/" + globalConfig.outputFileName);
